---
title: "Lab 9: Mini Project"
author: "Taylor Darby"
date: "10/27/2021"
output: pdf_document
---

# 1. Exploratory data analysis

## Preparing the data

```{r}
# Save input data file into Project directory
fna.data <- "WisconsinCancer.csv"

# read the csv file
wisc.d <- read.csv(fna.data, row.names = 1)

# examine input data
head(wisc.d)

# **WARNING** determined there was an issue with the file. A last empty column was inserted and needs to be removed.

ncol(wisc.d)
# There are 32 columns so we will remove the last column
wisc.df <- wisc.d[,-32]

ncol(wisc.df)
```

```{r}
# Remove the first column of the dataset because essentially this is our "answer" in our unsupervised analysis

wisc.data <- wisc.df[,-1]

# Setup a separate vector with data from the "diagnosis" column only ('as.factor()' function makes using the data easier later)
diagnosis <- as.factor(wisc.df$diagnosis)
diagnosis
```

## Exploratory data analysis

> **Q1.** How many observations are in this dataset?

### There are 569 observations in this dataset.

```{r}
nrow(wisc.data)
```

> **Q2.** How many of the observations have a malignant diagnosis?

### There are 212 malignant diagnosis.

```{r}
# generate a table of the "diagnosis" data to sum each categorical value
table(diagnosis)
```

> **Q3.** How many variables/features in the data are suffixed with '_mean'?

### There are 10 variables/features suffixed with '_mean'

```{r}
# use grep to determine where the pattern '_mean" occurs and use 'length' to count how many times the patter occurs

length(grep("_mean", colnames(wisc.df)))
```

# 2. Principal Component Analysis

## Performing PCA

```{r}
# Check the columns of the 'wisc.data' to determine if the data should be scaled

colMeans(wisc.data)

apply(wisc.data, 2, sd)
```

```{r}
# Perform PCA on wisc.data by completing the following code --> determined data needs to be scaled
wisc.pr <- prcomp(wisc.data, scale. = TRUE)
summary(wisc.pr)
```

> **Q4.** From your results, what proportion of the original variance is captured by the first principal components (PC1)?

### Proportion of Variance PC1 = 0.4427

> **Q5.** How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

### Three. In PC3 at least 70% of the original variance is described

> **Q6.** How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

### Seven. In PC7 at least 90% of the original variance is described

## Interpreting PCA results

```{r}
# Create a biplot of the PCA data (wisc.pr)
biplot(wisc.pr)
```

> **Q7.** What stands out to you about this plot? Is it easy or difficult to understand? Why?

### There are way too many datapoints to be able to understand what is going on.

Let's find a solution to this:
```{r}
# Generate a more standard scatter plot of each observation along principal components 1 and 2 and color the points by the diagnosis (Malignant = red and benign = black)

plot(wisc.pr$x, col=diagnosis, xlab="PC1", ylab="PC2")
  
```

> **Q8.** Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

### The plots look very similar but the PC3 values are shifted down a bit.

```{r}
# Repeat for components 1 and 3
plot(wisc.pr$x[,c("PC1","PC3")], col = diagnosis, 
     xlab = "PC1", ylab = "PC3")
```

Let's try in **ggplot**
```{r}
# Create a data.frame for ggplot
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

# Load the ggplot2 package
library(ggplot2)

# Make a scatter plot colored by diagnosis
ggplot(df) + 
  aes(PC1, PC2, col=diagnosis) + 
  geom_point()
```

## Variance explained

```{r}
# Calculate variance of each component
pr.var <- wisc.pr$sdev^2
head(pr.var)
```
Calculate the variance explained by each principal component by dividing by the total variance explained of all principal components.

```{r}
# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

## **OPTIONAL:** There are quite a few CRAN packages that are helpful for PCA. This includes the factoextra package. Feel free to explore this package. 

```{r}
# install.packages("factoextra")
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```

## Communicating PCA results

> **Q9.** For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?

### -0.2608538

```{r}
# call the first column of row "concave.points_mean" from the "rotation" dataset of 'wisc.pr'
wisc.pr$rotation["concave.points_mean",1]
```

> **Q10.** What is the minimum number of principal components required to explain 80% of the variance of the data?

### 4

There are four PCs with less than 80% variance of the data. That means it takes 5 PCs to explain 80% or more of the variance of the data.
```{r}
var <- summary(wisc.pr)
sum(var$importance[3,] < 0.8)

```

# 3. Hierarchical clustering

```{r}
# Scale the wisc.data data using the "scale()" function
data.scaled <- scale(wisc.data)
```


```{r}
# Calculate the (Euclidean) distances between all pairs of observations in the new scaled dataset and assign the result to data.dist.
data.dist <-  dist(data.scaled)
```

```{r}
# Create a hierarchical clustering model using complete linkage. Manually specify the method argument to 'hclust()' and assign the results to 'wisc.hclust'.
wisc.hclust <- hclust(data.dist, method = "complete")
```


## Results of hierarchical clustering

> **Q11.** Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

### At about a height of 19 the clustering model has 4 clusters.

```{r}
plot(wisc.hclust, main="Complete")
abline(h=19, col="red", lty=2)
```

Use 'cutree()' to cut the tree so that it has 4 clusters. Assign the output to the variable wisc.hclust.clusters.

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k = 4)
table(wisc.hclust.clusters, diagnosis)

```

> **Q12.** Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?

### Four clusters seems to be the best, but 6 clusters results in a third, very small group of 12 benign cells.

```{r}
wisc.hclust.clusters.2 <- cutree(wisc.hclust, k = 6)
table(wisc.hclust.clusters.2, diagnosis)

```

## Using different methods

> **Q13.** Which method gives your favorite results for the same 'data.dist' dataset? Explain your reasoning.

### I think the 'ward.D2' is the best method because we are plotting PCA results and 'ward.D2' clusters based on variance (is based on multidimensional variance) like PCA

```{r}
# "Single" method cut at 4 clusters
wisc.hclust.single <- hclust(data.dist, method = "single")
plot(wisc.hclust.single, main="Single")
abline(h=8.1 , col="red", lty=2)
```

```{r}
# "Average" method cut at 4 clusters
wisc.hclust.average <- hclust(data.dist, method = "average")
plot(wisc.hclust.average, main="Average")
abline(h=14 , col="red", lty=2)
```

```{r}
# "Ward.D2" method cut at 4 clusters
wisc.hclust.ward <- hclust(data.dist, method = "ward.D2")
plot(wisc.hclust.ward, main="Ward.D2")
abline(h=32.5 , col="red", lty=2)
```

> **Q14.** (OPTIONAL) - SKIPPED

# Started this section of Lab 9 the folowing class Oct. 29, 2021

## 5. Combining methods

I will use 4 PCs this time and 'hclust()' and 'dist()' as input

```{r}
wisc.pr.hclust <- hclust(dist(wisc.pr$x[,1:4]), method="ward.D2")
plot(wisc.pr.hclust, main="90% of the Variable Data")
abline(h=80 , col="red", lty=2)
```


Let's find our cluster membership vector by cutting this tree into k=2 groups.

```{r}
# two main branches of or dendrogram indicating two main clusters - maybe these are malignant and benign?
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```

Now let's compare to the expert M and B vector

```{r}
table(diagnosis)
```

> **Q15.** How well does the newly created model with four clusters separate out the two diagnoses?

### Very well. See table below.

We can do a cross-table by giving the 'table()' function to two inputs.

```{r}
table(grps, diagnosis)
```


> **Q16.** How well do the k-means and hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.km$cluster and wisc.hclust.clusters) with the vector containing the actual diagnoses.

### I didn't do the optional k-means section but here you can see before ward.D2 we need to generate 4 groups before seeing separation. After ward.D2 the separation is clearer.

```{r}
table(wisc.hclust.clusters, diagnosis)
table(grps, diagnosis)
```


## 6. Sensitivity/Specificity

**Accuracy:** essentially how many did we get correct?

```{r}
# pre-ward.d2
(165+343) / nrow(wisc.data)

# post-ward.d2
(165+351) / nrow(wisc.data)
```

> **Q17.** Which of your analysis procedures resulted in a clustering model with the best specificity? How about sensitivity?

### PCA scaled and grouped into 2 groups using ward.D2 is more specific, but not as sensitive as the data before ward.D2 analysis.

**Sensitivity** refers to a test’s ability to correctly detect ill patients who do have the condition. In our example here the sensitivity is the total number of samples in the cluster identified as predominantly malignant (cancerous) divided by the total number of known malignant samples. In other words: TP/(TP+FN).

```{r}
# pre-ward.d2
(165/(165+12))
# post-ward.d2
(165/(165+6))
```

**Specificity** relates to a test’s ability to correctly reject healthy patients without a condition. In our example specificity is the proportion of benign (not cancerous) samples in the cluster identified as predominantly benign that are known to be benign. In other words: TN/(TN+FP).

```{r}
# pre-ward.d2
(343/(343+40))
# post-ward.d2
(351/(351+47))
```


## 7. Prediction

We will use the predict() function that will take our PCA model from before and new cancer cell data and project that data onto our PCA space.

```{r}
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```

Now add these new samples to our PCA plot
```{r}
plot(wisc.pr$x[,1:2], col=diagnosis)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```

> **Q18.** Which of these new patients should we prioritize for follow up based on your results?

### I would follow up with patient 2 because the analysis predicted their test results were consistent with the malignant profile according to our previous PCA analysis.

```{r}
sessionInfo()
```

