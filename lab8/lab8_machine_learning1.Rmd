---
title: "Lab 8: Machine Learning 1"
author: "Taylor Darby"
date: "10/22/2021"
output: pdf_document
---

# Clustering methods

Kmeans clustering in R is done with the "kmeans()" function.
Here we makeup some data to test and learn with.

```{r}
# 'rnorm' provides a random dataset distributed normally, and specifying x and y sets the number of rows and the distance between the two columns.
tmp <- c(rnorm(30, 3), rnorm(30, -3))

# 'cbind' binds the data in the columns together
data <- cbind(x=tmp, y=rev(tmp))

# We now have a dataset to perform k-means with
data
```

Run 'kmeans()':
Set "k"("centers") to 2 nstart to 20. The thing with 'kmeans' is that you have to tell it how many clusters you want (in this case = 2)

```{r}
km <- kmeans(data, centers = 2, nstart = 20)
km
```

> Q1. How many points are in each cluster?

```{r}
km$size

```

> Q2. What 'component' of your result object details cluster assignment/membership?

```{r}
km$cluster
```


> Q3. What 'component' of your result object details cluster center?

```{r}
km$centers
```


> Q4. Plot x colored by the kmeans cluster assignment and add cluster centers as blue points

```{r}
plot(data, col=km$cluster)
points(km$centers, col="#05DCBB", pch=15, cex=2)
```

# Hierarchical Clustering

We will use the 'hclust()' function on the same data as before and see how this method works.

```{r}
hc <- hclust(dist(data))
hc
```

'hclust' gas a plot method

```{r}
plot(hc, cex=0.5)
abline(h=7, col="#05DCBB")
```

To find our membership vector we need to "cut" the tree and for this we use the 'cutree()' function and tell it the height to cut at.

```{r}
cutree(hc, h=7)
```

We can also use 'cutree()' and state the number of k clusters we want...

```{r}
grps <- cutree(hc, k=2)

plot(data, col=grps)
```

# Principal Component Analysis (PCA)

PCA is useful for visualizing key variance in datasets with high dimensionality.

## PCA of UK food data

Import UK food dataset

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)
head(x)
```

> **Q1.** How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?

```{r}
## Complete the following code to find out how many rows and columns are in x?
dim(x)
```
```{r}
## Preview the first 6 rows
head(x)
```

> **Q2.** Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?

```{r}
# Note how the minus indexing works
# rownames(x) <- x[,1] "This is not a good way to code because you will lose a column every time you run"
# x <- x[,-1]

# This way is better
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url, row.names=1)
head(x)
```


## Spotting major differences and trends

Let's plot the data

```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```

> **Q3:** Changing what optional argument in the above barplot() function results in the following plot?

```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
```

> **Q5:** Generating all pairwise plots may help somewhat. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?

Make a pairwise plot

```{r}
mycols <- rainbow( nrow(x))
pairs(x, col=mycols, pch=16)
```

**Answer:**The axis comparisons change across rows and columns. If the point lies on the diagonal then that food is consumed at the same rate in both countries.


> **Q6.** What is the main differences between N. Ireland and the other countries of the UK in terms of this data-set?

N. Ireland consumes less of one of the values than the rest of the countries.

## PCA to the rescue

Here we will use the base R function for PCA, which is called 'prcomp()'. Note: This function wants the transpose of our data.

```{r}
# Use the prcomp() PCA function 
pca <- prcomp( t(x) )
summary(pca)
```

Plot this pca
```{r}
plot(pca)
```

> **Q7.** Complete the code below to generate a plot of PC1 vs PC2. The second line adds text labels over the data points.

```{r}
# Plot PC1 vs PC2
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pca$x[,1], pca$x[,2], colnames(x))
```

> **Q8.** Customize your plot so that the colors of the country names match the colors in our UK and Ireland map and table at start of this document.

```{r}
# Plot PC1 vs PC2
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pca$x[,1], pca$x[,2], labels = colnames(x), col=c("orange", "red", "blue", "green"))
```

## Below we can use the square of pca$sdev , which stands for “standard deviation”, to calculate how much variation in the original data each PC accounts for.

```{r}
v <- round( pca$sdev^2/sum(pca$sdev^2) * 100 )
v

## or the second row here...
z <- summary(pca)
z$importance
```

## This information can be summarized in a plot of the variances (eigenvalues) with respect to the principal component number (eigenvector number), which is given below.

```{r}
barplot(v, xlab="Principal Component", ylab="Percent Variation")
```

## Digging deeper (variable loadings)

```{r}
## Lets focus on PC1 as it accounts for > 90% of variance 
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )
```

> **Q9:** Generate a similar ‘loadings plot’ for PC2. What two food groups feature prominantely and what does PC2 maninly tell us about?





## One more PCA for today

Import RNAseq data
```{r}
url2 <- "https://tinyurl.com/expression-CSV"
rna.data <- read.csv(url2, row.names=1)
head(rna.data)
```


> **Q10:** How many genes and samples are in this data set?

```{r}
nrow(rna.data)
```

```{r}
## Again we have to take the transpose of our data 
pca.rna <- prcomp(t(rna.data), scale=TRUE)
 
## Simple un polished plot of pc1 and pc2
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2")

summary(pca.rna)
```

Scree plot of RNA data

```{r}
plot(pca.rna$x[,1:2])
text(pca.rna$x[,1:2], labels=colnames(rna.data))
```

**OR** Let’s make the above scree plot ourselves and in so doing explore the object returned from prcomp() a little further. We can use the square of pca$sdev, which stands for “standard deviation”, to calculate how much variation in the original data each PC accounts for:

```{r}
## Variance captured per PC 
pca.var <- pca.rna$sdev^2

## Percent variance is often more informative to look at 
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
pca.var.per
```
We can use this to generate our own scree-plot like this

```{r}
barplot(pca.var.per, main="Scree Plot", 
        names.arg = paste0("PC", 1:10),
        xlab="Principal Component", ylab="Percent Variation")
```

Now lets make our main PCA plot a bit more attractive and useful

```{r}
## A vector of colors for wt and ko samples
colvec <- colnames(rna.data)
colvec[grep("wt", colvec)] <- "red"
colvec[grep("ko", colvec)] <- "blue"

plot(pca.rna$x[,1], pca.rna$x[,2], col=colvec, pch=16,
     xlab=paste0("PC1 (", pca.var.per[1], "%)"),
     ylab=paste0("PC2 (", pca.var.per[2], "%)"))

text(pca.rna$x[,1], pca.rna$x[,2], labels = colnames(rna.data), pos=c(rep(4,5), rep(2,5)))
```

... left off on ggplot section

